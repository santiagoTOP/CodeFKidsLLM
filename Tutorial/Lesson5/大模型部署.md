## 大模型部署

![image-20240403102737257](http://typora-picture-room.oss-cn-chengdu.aliyuncs.com/img/image-20240403102737257.png)

![image-20240403104313551](http://typora-picture-room.oss-cn-chengdu.aliyuncs.com/img/image-20240403104313551.png)

### LMDeploy

![image-20240403104852883](http://typora-picture-room.oss-cn-chengdu.aliyuncs.com/img/image-20240403104852883.png)

![image-20240403105443790](http://typora-picture-room.oss-cn-chengdu.aliyuncs.com/img/image-20240403105443790.png)

![image-20240403105723038](http://typora-picture-room.oss-cn-chengdu.aliyuncs.com/img/image-20240403105723038.png)

![image-20240403105959834](http://typora-picture-room.oss-cn-chengdu.aliyuncs.com/img/image-20240403105959834.png)

![image-20240403110210839](http://typora-picture-room.oss-cn-chengdu.aliyuncs.com/img/image-20240403110210839.png)

![image-20240403110423339](http://typora-picture-room.oss-cn-chengdu.aliyuncs.com/img/image-20240403110423339.png)

![image-20240403110550850](http://typora-picture-room.oss-cn-chengdu.aliyuncs.com/img/image-20240403110550850.png)

![image-20240403110701886](http://typora-picture-room.oss-cn-chengdu.aliyuncs.com/img/image-20240403110701886.png)

![image-20240403111007686](http://typora-picture-room.oss-cn-chengdu.aliyuncs.com/img/image-20240403111007686.png)

![image-20240403111314085](http://typora-picture-room.oss-cn-chengdu.aliyuncs.com/img/image-20240403111314085.png)

![image-20240403111512780](http://typora-picture-room.oss-cn-chengdu.aliyuncs.com/img/image-20240403111512780.png)

## 模型服务架构

![lmdeploy.drawio.png](https://github.com/InternLM/Tutorial/blob/main/lmdeploy/img/lmdeploy.drawio.png?raw=true)

## 模型转换

- 使用 TurboMind 推理模型需要先将模型转化为 TurboMind 的格式

### 在线转换

- 三种类型

  - 加载在huggingface上已经转换好的TurboMind格式权重

    `lmdeploy chat turbomind internlm/internlm-chat-20b-4bit --model-name internlm-chat-20b`

  - 加载其他的格式权重

    `lmdeploy chat turbomind Qwen/Qwen-7B-Chat --model-name qwen-7b`

  - 直接启动本地的 Huggingface 模型

    `lmdeploy chat turbomind /share/temp/model_repos/internlm-chat-7b/  --model-name internlm-chat-7b`

### 离线转换

- 离线转换需要在启动服务之前，将模型转为 lmdeploy TurboMind 的格式，在本地生成一个workspace文件

  `lmdeploy convert internlm-chat-7b  /root/share/temp/model_repos/internlm-chat-7b/`

## 模型服务

### TurboMind 推理+命令行本地对话

- 直接用转换后的模型进行推理服务

  `lmdeploy chat turbomind ./workspace`

### TurboMind推理+API服务

- 在模型的基础上提供外API服务

```
# 开启服务

lmdeploy serve api_server ./workspace \
	--server_name 0.0.0.0 \
	--server_port 23333 \
	--instance_num 64 \
	--tp 1   # tp 表示只有一张卡
```

```
# 调用服务

lmdeploy serve api_client http://localhost:23333
```

### TurboMind推理+API服务+Gradio

```
# 前提是要开启API服务

lmdeploy serve gradio http://0.0.0.0:23333 \
	--server_name 0.0.0.0 \
	--server_port 6006 \
	--restful_api True
```

### TurboMind推理+Gradio

```
lmdeploy serve gradio ./workspace
```

### TurboMind 推理 + Python 

```
from lmdeploy import turbomind as tm

# load model
model_path = "/root/share/temp/model_repos/internlm-chat-7b/"
tm_model = tm.TurboMind.from_pretrained(model_path, model_name='internlm-chat-20b')
generator = tm_model.create_instance()

# process query
query = "你好啊兄嘚"
prompt = tm_model.model.get_prompt(query)
input_ids = tm_model.tokenizer.encode(prompt)

# inference
for outputs in generator.stream_infer(
        session_id=0,
        input_ids=[input_ids]):
    res, tokens = outputs[0]

response = tm_model.tokenizer.decode(res.tolist())
print(response)
```

## 模型量化

- 针对Decoder Only访存密集问题使用 **KV Cache 量化**和 **4bit Weight Only 量化（W4A16）**
- KV Cache 量化是指将逐 Token（Decoding）生成过程中的上下文 K 和 V 中间结果进行 INT8 量化（计算时再反量化），以降低生成过程中的显存占用
- 4bit Weight 量化，将 FP16 的模型权重量化为 INT4，Kernel 计算时，访存量直接降为 FP16 模型的 1/4，大幅降低了访存成本，计算时反量化为FP16

### KV Cache 量化

- 第一步：计算 minmax。主要思路是通过计算给定输入样本在每一层不同位置处计算结果的统计情况。

```
lmdeploy lite calibrate \
  --model  /root/share/temp/model_repos/internlm-chat-7b/ \
  --calib_dataset "c4" \   # 使用C4数据去获取参数中我们需要的统计值
  --calib_samples 128 \
  --calib_seqlen 2048 \
  --work_dir ./quant_output
```

- 第二步：通过 minmax 获取量化参数

```
zp = (min+max) / 2
scale = (max-min) / 255
quant: q = round( (f-zp) / scale)
dequant: f = q * scale + zp

lmdeploy lite kv_qparams \
  --work_dir ./quant_output  \
  --turbomind_dir workspace/triton_models/weights/ \
  --kv_sym False \
  --num_tp 1
```

- 第三步：修改配置，修改参数配置中的quant_policy=4
  - 在接下来的模型运行过程中会对产生的KV进行量化处理

### W4A16 量化

- 第一步：计算 minmax。主要思路是通过计算给定输入样本在每一层不同位置处计算结果的统计情况。

```
lmdeploy lite calibrate \
  --model  /root/share/temp/model_repos/internlm-chat-7b/ \
  --calib_dataset "c4" \   # 使用C4数据去获取参数中我们需要的统计值
  --calib_samples 128 \
  --calib_seqlen 2048 \
  --work_dir ./quant_output
```

- 第二步：量化权重模型。利用第一步得到的统计值对参数进行量化，具体又包括两小步：

  - 缩放参数。主要是性能上的考虑。

  - 整体量化。

```
# 量化权重模型
lmdeploy lite auto_awq \
  --model  /root/share/temp/model_repos/internlm-chat-7b/ \
  --w_bits 4 \
  --w_group_size 128 \
  --work_dir ./quant_output   # 量化后的模型输出
```

- 第三步：转换成 TurboMind 格式

```
lmdeploy convert  internlm-chat-7b ./quant_output \
    --model-format awq \
    --group-size 128 \
    --dst_path ./workspace_quant
```

